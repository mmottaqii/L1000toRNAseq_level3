{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmottaqi/.conda/envs/mcf7/lib/python3.11/site-packages/loompy/bus_file.py:67: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "/home/vmottaqi/.conda/envs/mcf7/lib/python3.11/site-packages/loompy/bus_file.py:84: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n",
      "/home/vmottaqi/.conda/envs/mcf7/lib/python3.11/site-packages/loompy/bus_file.py:101: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import loompy as lp\n",
    "#import crick\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import h5py\n",
    "import tdigest\n",
    "from tdigest import TDigest\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c4b25b",
   "metadata": {},
   "source": [
    "# 1. Checking the original CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa82fe8",
   "metadata": {},
   "source": [
    "## head, shape and subset printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"original.csv\")\n",
    "\n",
    "rows = df.shape[0]\n",
    "columns = df.shape[1] \n",
    "print(f\"rows = {rows}\")\n",
    "print(f\"columns = {columns}\")\n",
    "print()\n",
    "\n",
    "print(\"columns:\")\n",
    "print(df.columns[:17])\n",
    "\n",
    "df.head()\n",
    "\n",
    "subset = df.iloc[:3, 0:17]\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c767e",
   "metadata": {},
   "source": [
    "## Checking unique values of cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad044fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"A375_SMILES_siginfo.pkl\")\n",
    "\n",
    "columns = df.columns[:18]\n",
    "\n",
    "# Count\n",
    "columns_unique = df[columns].nunique()\n",
    "# The nunique() function in pandas is used to count the number of unique values in a pandas Series or DataFrame. \n",
    "\n",
    "# Create a file to save the output\n",
    "output_file = \"1_nunique.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    for name, count in columns_unique.items():\n",
    "        output = f\"{name}: {count} unique values\"\n",
    "        file.write(output + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3123e7",
   "metadata": {},
   "source": [
    "## Printing idose and itime values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77010572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"A375_SMILES_siginfo.pkl\")\n",
    "\n",
    "selected_column_dose = df[\"pert_idose\"]\n",
    "# Get unique row values in the column\n",
    "unique_dose = selected_column_dose.unique()\n",
    "\n",
    "selected_column_time = df[\"pert_itime\"]\n",
    "unique_time = selected_column_time.unique()\n",
    "\n",
    "# Create a new TXT file and write unique values\n",
    "print(f\"Unique idose: {unique_dose}\")\n",
    "print()\n",
    "print(f\"Unique time: {unique_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea242a",
   "metadata": {},
   "source": [
    "## Pert id and name differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ebad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"A375_SMILES_siginfo.pkl\")\n",
    "\n",
    "unique_values_x = df['pert_id'].unique()\n",
    "\n",
    "# Filter the DataFrame based on unique values in column 'x'\n",
    "filtered_df = df[df['pert_id'].isin(unique_values_x)]\n",
    "\n",
    "# Select only columns 'x' and 'y' from the filtered DataFrame\n",
    "filtered_df = filtered_df[['pert_id', 'pert_name']]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "filtered_df.to_csv('2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4a8a9",
   "metadata": {},
   "source": [
    "## Number of zeros and NaNs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "total_cells = df.size\n",
    "\n",
    "# Count the number of cells with zero\n",
    "zero_cells = np.count_nonzero(df == 0)\n",
    "\n",
    "# Count the number of cells with NaN values\n",
    "nan_cells = df.isnull().sum().sum()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Total number of cells:\", total_cells)\n",
    "print(\"Number of cells with zero:\", zero_cells)\n",
    "print(\"Number of cells with NaN values:\", nan_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70996c0",
   "metadata": {},
   "source": [
    "## Checking for duplicate cols (genes) and rows (samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c92a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this works, rm the next two blocks\n",
    "\n",
    "df = pd.read_csv('original.csv')\n",
    "\n",
    "# Check for duplicate columns\n",
    "duplicate_cols = df.columns[df.T.duplicated()].tolist()\n",
    "duplicate_cols_indices = [df.columns.get_loc(col) for col in duplicate_cols]\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df[df.duplicated()].index.tolist()\n",
    "\n",
    "# Write the results to a text file\n",
    "with open('3_duplicates.txt', 'w') as file:\n",
    "    file.write('duplicate columns (genes):\\n')\n",
    "    file.write(', '.join(duplicate_cols) + '\\n')\n",
    "    file.write(', '.join(map(int, duplicate_cols_indices)) + '\\n')\n",
    "    file.write(f'len: {len(duplicate_cols)}\\n\\n')\n",
    "    \n",
    "    file.write('duplicate rows (samples):\\n')\n",
    "    file.write(', '.join([f'row{i}' for i in duplicate_rows]) + '\\n')\n",
    "    file.write(', '.join(map(str, duplicate_rows)) + '\\n')\n",
    "    file.write(f'len: {len(duplicate_rows)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f039860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "\n",
    "# Check for duplicate names in the columns\n",
    "duplicates_in_columns = df.columns.duplicated()\n",
    "\n",
    "# Get the names of the duplicate columns, if any\n",
    "duplicate_column_names = df.columns[duplicates_in_columns]\n",
    "\n",
    "if len(duplicate_column_names) > 0:\n",
    "    print(\"Duplicate column names found:\")\n",
    "    print(duplicate_column_names)\n",
    "else:\n",
    "    print(\"No duplicate column names found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "\n",
    "# Initialize a list to store the names of columns with only one unique value\n",
    "single_value_columns = []\n",
    "\n",
    "# Iterate over each column and check the number of unique values\n",
    "for column in df.columns:\n",
    "    if df[column].nunique() == 1:\n",
    "        single_value_columns.append(column)\n",
    "\n",
    "with open('15_unique_genes.txt', 'w') as f:\n",
    "    for column in single_value_columns:\n",
    "        value = df.loc[1, column]  # get the second-row value (indexing starts from 0)\n",
    "        f.write(f\"{column}: {value}\\n\")\n",
    "                \n",
    "print(single_value_columns)\n",
    "print(len(single_value_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c396693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not neccessary\n",
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "\n",
    "unique_values_count = df[\"MSRA_cellline\"].nunique()\n",
    "unique_values_count0 = df[\"ANKRD10_cellline\"].nunique()\n",
    "\n",
    "print(f\"Number of unique values in column 'MSRA_cellline': {unique_values_count}\")\n",
    "print(f\"Number of unique values in column 'ANKRD10_cellline': {unique_values_count0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c70df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not neccessary\n",
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "print(df[\"A4GNT\"])\n",
    "print(df[\"ATP5G2P3\"])\n",
    "print(df[\"HCRTR2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890a668",
   "metadata": {},
   "source": [
    "## Histogram of genes (cols) with one unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is written in second part (cell )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324b411",
   "metadata": {},
   "source": [
    "## Checking for string values in cols(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b911f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('original.csv')\n",
    "\n",
    "# Extract the cell attributes from columns 0 to 22 (including)\n",
    "cell_attributes_df = df.iloc[:, :16]\n",
    "\n",
    "# Extract the gene expression values from columns 23 onwards\n",
    "gene_expression_df = df.iloc[:, 16:]\n",
    "gene_expression_df = gene_expression_df.replace([np.nan, np.inf, -np.inf], -1)\n",
    "\n",
    "gene_expression_data = gene_expression_df.values.T\n",
    "#gene_expression_data = gene_expression_data.astype(float)\n",
    "\n",
    "gene_expression_list = gene_expression_data.tolist()\n",
    "\n",
    "# Iterate through the list and print the string values\n",
    "for row in gene_expression_list:\n",
    "    for value in row:\n",
    "        if isinstance(value, str):\n",
    "            print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daace1f",
   "metadata": {},
   "source": [
    "## Getting loc of a string col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51746fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "\n",
    "# Find the index of the column \"cpd\"\n",
    "cpd_index = df.columns.get_loc(\"cid\")\n",
    "\n",
    "print(cpd_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10f1e0",
   "metadata": {},
   "source": [
    "## 978 landmark genes check difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('978_L1000.txt', 'r') as txt_file:\n",
    "    txt_lines = txt_file.readlines()\n",
    "\n",
    "# Extract values from the second column\n",
    "txt_column_2 = [line.split()[1] for line in txt_lines]\n",
    "#print(txt_column_2)\n",
    "\n",
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "csv_column_names = df.columns[16:]\n",
    "\n",
    "# Compare the names from the txt file with the csv column names\n",
    "overlap_names = set(csv_column_names) & set(txt_column_2)\n",
    "\n",
    "non_overlap_names = set(csv_column_names) - overlap_names\n",
    "#non_overlap_l1000 = [elem for elem in set(txt_column_2) if elem not in set(csv_column_names)]\n",
    "not_overlap = set(txt_column_2) - overlap_names\n",
    "\n",
    "print(\"Number of overlapping names:\", len(overlap_names))\n",
    "print(\"Number of non-overlapping names:\", len(non_overlap_names))\n",
    "print(non_overlap_names)\n",
    "print(\"L1000 genes not in the dataset:\")\n",
    "print(not_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dea856",
   "metadata": {},
   "source": [
    "## Indices of columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not neccessary\n",
    "\n",
    "df = pd.read_csv(\"5_sorted_head.csv\", engine=\"python\")\n",
    "\n",
    "column_names = ['pert_iname', 'pert_idose', 'pert_itime']\n",
    "# Get the indices of the columns\n",
    "column_indices = [df.columns.get_loc(col_name) for col_name in column_names]\n",
    "\n",
    "print(column_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933f628",
   "metadata": {},
   "source": [
    "# Sorting dataset for replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0fb09",
   "metadata": {},
   "source": [
    "## pkl to csv (sorting operation) (only if input is pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ad0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"A375_SMILES_siginfo.pkl\")\n",
    "\n",
    "df['pert_idose'] = df['pert_idose'].str.replace(' um', '').astype('float64')\n",
    "df['pert_itime'] = df['pert_itime'].str.replace(' h', '').astype('float64')\n",
    "\n",
    "# Sort the dataset by multiple columns\n",
    "sorted_df = df.sort_values(by=['pert_id', 'pert_idose', 'pert_itime'])\n",
    "\n",
    "# Reset the index with a new column\n",
    "sorted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the sorted data to a new CSV file\n",
    "sorted_df.to_csv('3_sorted_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2bd4b",
   "metadata": {},
   "source": [
    "## Sorting original csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"original.csv\")\n",
    "\n",
    "df['pert_idose'] = df['pert_idose'].str.replace(' um', '').astype('float64')\n",
    "df['pert_itime'] = df['pert_itime'].str.replace(' h', '').astype('float64')\n",
    "\n",
    "# Sort the dataset by multiple columns\n",
    "sorted_df = df.sort_values(by=['pert_id', 'pert_idose', 'pert_itime'])\n",
    "\n",
    "# Reset the index with a new column\n",
    "sorted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the sorted data to a new CSV file\n",
    "sorted_df.to_csv('3_sorted_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef662c8",
   "metadata": {},
   "source": [
    "## Printing dataset characteristics & making subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"4_sorted_dataset.csv\")\n",
    "\n",
    "num_rows = df.shape[0]\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "print(\"columns:\")\n",
    "print(df.columns[:17])\n",
    "print()\n",
    "subset = df.iloc[:3, 0:17]\n",
    "print(subset)\n",
    "\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)\n",
    "\n",
    "first_200_rows = df.head(200)\n",
    "first_200_rows.to_csv('5_sorted_head.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0d068",
   "metadata": {},
   "source": [
    "## Indices of replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Initialize variables\n",
    "current_val = None\n",
    "indices = []\n",
    "output_lines = []\n",
    "\n",
    "# Open the CSV file for reading\n",
    "with open(\"3_sorted_dataset.csv\", \"r\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)  # Create a CSV reader\n",
    "\n",
    "    for index, row in enumerate(csv_reader):\n",
    "        pert_iname = row[\"pert_iname\"]\n",
    "        pert_idose = float(row[\"pert_idose\"])\n",
    "        pert_itime = float(row[\"pert_itime\"])\n",
    "\n",
    "        if (pert_iname, pert_idose, pert_itime) == current_val:\n",
    "            indices.append(str(index))  # Convert index to string and append\n",
    "        else:\n",
    "            if current_val is not None:\n",
    "                output_lines.append(\",\".join(indices))\n",
    "\n",
    "            current_val = (pert_iname, pert_idose, pert_itime)\n",
    "            indices = [str(index)]\n",
    "\n",
    "    if indices:\n",
    "        output_lines.append(\",\".join(indices))\n",
    "\n",
    "# Write the output to a file\n",
    "with open('8_sorted_indices.txt', 'w') as file:\n",
    "    for line in output_lines:\n",
    "        file.write(f\"{line}\\n\")\n",
    "    \n",
    "    num_lines = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of replicates:\", num_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d138c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "\n",
    "with open('8_sorted_indices.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()  # Remove leading/trailing whitespace\n",
    "        line_list = line.split(',')  # Split the line into a list using comma as the delimiter\n",
    "        line_list = [int(item) for item in line_list]\n",
    "        indices.append(line_list)\n",
    "\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192fa60",
   "metadata": {},
   "source": [
    "## Checking max and second-max in replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587191aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file and split lines by commas\n",
    "with open(\"8_sorted_indices.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Convert the text values to integers\n",
    "data = [[int(num) for num in line.strip().split(\",\")] for line in lines]\n",
    "\n",
    "# Find the maximum count of numbers in a row\n",
    "max_count = max(len(row) for row in data)\n",
    "\n",
    "# Calculate the lengths of each row\n",
    "row_lengths = [len(row) for row in data]\n",
    "# Sort the row_lengths in descending order\n",
    "row_lengths.sort(reverse=True)\n",
    "second_max = row_lengths[1]\n",
    "\n",
    "print(\"Maximum count of numbers in a replicate:\", max_count)\n",
    "print(f\"Second max count of numbers in a replicate: {second_max}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c41c0",
   "metadata": {},
   "source": [
    "### Getting description of max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = 0\n",
    "max_indices = []\n",
    "\n",
    "with open(\"8_sorted_indices.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        numbers = list(map(int, line.strip().split(',')))\n",
    "        if len(numbers) > max_count:\n",
    "            max_count = len(numbers)\n",
    "            max_indices = numbers\n",
    "\n",
    "# Read the csv file using pandas\n",
    "df = pd.read_csv(\"3_sorted_dataset.csv\")\n",
    "\n",
    "# Use the indices to get the rows and the first 15 columns\n",
    "selected_rows = df.iloc[max_indices, :15]\n",
    "\n",
    "# Save the result to a txt file\n",
    "with open(\"8.1_max_replicate_des.txt\", \"w\") as f:\n",
    "    # Write the header\n",
    "    f.write(\"indice, \" + ', '.join(selected_rows.columns) + \"\\n\")\n",
    "\n",
    "    for index, row in selected_rows.iterrows():\n",
    "        f.write(str(index) + \", \" + ', '.join(map(str, row)) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b549e3",
   "metadata": {},
   "source": [
    "### Sorting the max replicate with sig_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '3_sorted_dataset.csv'\n",
    "output_file = '8.5_replicate.csv'\n",
    "\n",
    "# The row indices to select\n",
    "start_row = 21378\n",
    "end_row = 22016\n",
    "\n",
    "# Read specific rows from the CSV file into a DataFrame\n",
    "df_selected_rows = pd.read_csv(input_file, skiprows=range(1, start_row), nrows=(end_row - start_row + 1))\n",
    "\n",
    "unique_count = df_selected_rows['sig_id'].nunique()\n",
    "\n",
    "print(f\"The number of unique values in the 'sig_id' column is {unique_count}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945963ea",
   "metadata": {},
   "source": [
    "### Changing the indices file based on the new criterion (to be written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4d78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daff6b5d",
   "metadata": {},
   "source": [
    "# Pearson Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361f0dc",
   "metadata": {},
   "source": [
    "## small PC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20079ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"4_sorted_head.pkl\")\n",
    "\n",
    "# Select the columns of interest (from column 23 onwards)\n",
    "gene_columns = df.columns[16:]\n",
    "\n",
    "#replicates = df.loc[:4, gene_columns].apply(pd.to_numeric, errors='coerce')\n",
    "df1 = df.loc[:3,gene_columns].apply(pd.to_numeric, errors='coerce').dropna(axis=1)\n",
    "\n",
    "pairwise_correlation = np.corrcoef(df1, rowvar=True)\n",
    "print(pairwise_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1fbf4",
   "metadata": {},
   "source": [
    "## Medium PC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a73a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT Necessary\n",
    "\n",
    "df = pd.read_pickle(\"4_sorted_dataset.pkl\")\n",
    "\n",
    "# Select the columns of interest (from column 23 onwards)\n",
    "gene_columns = df.columns[16:]\n",
    "df1 = df.loc[:, gene_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "def pairwise(vector):\n",
    "    for i in range(len(vector) - 1):\n",
    "        for j in range(i+1, len(vector)):\n",
    "            # Get the two rows for comparison\n",
    "            row1 = df1.loc[vector[i], gene_columns].astype(float)\n",
    "            row2 = df1.loc[vector[j], gene_columns].astype(float)\n",
    "\n",
    "            # Drop columns with NaN values from both rows\n",
    "            non_nan_columns = row1.notna() & row2.notna()\n",
    "            row1_dropped = row1[non_nan_columns]\n",
    "            row2_dropped = row2[non_nan_columns]\n",
    "\n",
    "            # Compute the correlation coefficient for the non-NaN values\n",
    "            correlation = round(np.corrcoef(row1_dropped, row2_dropped)[0, 1], 1)\n",
    "\n",
    "            # Store the correlation coefficient in the matrix\n",
    "            pairwise_correlation[vector[j], vector[i]] = correlation\n",
    "            \n",
    "# Initialize an empty correlation matrix\n",
    "pairwise_correlation = np.full((15, 15), 33.00)\n",
    "\n",
    "indices = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "        \n",
    "for elem in indices:\n",
    "    pairwise(elem)\n",
    "\n",
    "# Save the correlation matrix to a file\n",
    "np.savetxt('9_test_pairwise.csv', pairwise_correlation, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d7934",
   "metadata": {},
   "source": [
    "## Pairwise Pearson CORR operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def pairwise(vector, matrix, chunk_df):\n",
    "    for i in range(len(vector) - 1):\n",
    "        for j in range(i + 1, len(vector)):\n",
    "            row1 = chunk_df.loc[vector[i], :]\n",
    "            row2 = chunk_df.loc[vector[j], :]\n",
    "            non_nan_columns = row1.notna() & row2.notna()\n",
    "            row1_dropped = row1[non_nan_columns]\n",
    "            row2_dropped = row2[non_nan_columns]\n",
    "            correlation = round(np.corrcoef(row1_dropped, row2_dropped)[0, 1], 3)\n",
    "            matrix[vector[j], vector[i]] = correlation\n",
    "\n",
    "indices = []\n",
    "with open('8_sorted_indices.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        line_list = line.split(',')\n",
    "        line_list = [int(item) for item in line_list]\n",
    "        indices.append(line_list)\n",
    "\n",
    "chunksize = 3000\n",
    "pairwise_correlation = np.zeros((31680, 31680))\n",
    "prev_chunk_df = None\n",
    "\n",
    "# Read the CSV file chunk-by-chunk\n",
    "for start_idx in range(0, 31680, chunksize):\n",
    "    chunk = pd.read_csv(\"3_sorted_dataset.csv\", skiprows=range(1, start_idx + 1), nrows=chunksize)\n",
    "    actual_rows = chunk.shape[0]  # Get the actual number of rows in the chunk\n",
    "    end_idx = start_idx + actual_rows - 1  # Adjust the end index based on the actual number of rows\n",
    "    \n",
    "    gene_columns = chunk.columns[16:]\n",
    "    chunk_df = chunk.loc[:, gene_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    chunk_df.index = range(start_idx, end_idx + 1)  # Set the index based on the actual number of rows\n",
    "\n",
    "    \n",
    "    # Combine with previous chunk if it exists\n",
    "    if prev_chunk_df is not None:\n",
    "        combined_df = pd.concat([prev_chunk_df, chunk_df])\n",
    "    else:\n",
    "        combined_df = chunk_df\n",
    "\n",
    "    # Find the groups that overlap with this chunk's index range\n",
    "    relevant_indices = [grp for grp in indices if any(start_idx <= x <= end_idx for x in grp)]\n",
    "    \n",
    "    for elem in relevant_indices:\n",
    "        if all(x in combined_df.index for x in elem):\n",
    "            pairwise(elem, pairwise_correlation, combined_df)\n",
    "    \n",
    "    prev_chunk_df = chunk_df\n",
    "\n",
    "# Save the resulting matrix\n",
    "with h5py.File('10_pairwise_correlation.h5', 'w') as hf:\n",
    "    hf.create_dataset('pairwise_correlation', data=pairwise_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d367915",
   "metadata": {},
   "source": [
    "## test h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca422cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('10_pairwise_correlation.h5', 'r') as hf:\n",
    "    pairwise_correlation = hf['pairwise_correlation'][:8,:8]\n",
    "\n",
    "print(pairwise_correlation.shape[0])\n",
    "print()\n",
    "print(pairwise_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31544ae",
   "metadata": {},
   "source": [
    "# Assessing PC h5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d82b5",
   "metadata": {},
   "source": [
    "## Checking values in matrix rows and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('10_pairwise_correlation.h5', 'r') as hf:\n",
    "    dataset = hf['pairwise_correlation'][:]\n",
    "\n",
    "index = 0\n",
    "    \n",
    "# Count the number of non-zero values in the selected row\n",
    "row_i = dataset[index, :]\n",
    "col_i = dataset[:, index]\n",
    "\n",
    "\n",
    "count_row = np.count_nonzero(row_i)\n",
    "count_column = np.count_nonzero(col_i)\n",
    "\n",
    "\n",
    "# Count the number of values under 0.85 in row i\n",
    "num_values_under_085_row = np.count_nonzero((row_i < 0.99) & (row_i != 0))\n",
    "num_values_under_085_col = np.count_nonzero((col_i < 0.99) & (col_i != 0))\n",
    "\n",
    "#print(f\"Length: {len(num_values)}\")\n",
    "print(\"Row:\")\n",
    "print(f\"Non-zero in row {index}: {count_row}\")\n",
    "print(f\"Non-zero in column {index}: {count_column}\")\n",
    "print()\n",
    "\n",
    "print(f\"Under threshold in row: {num_values_under_085_row}\")\n",
    "print(f\"Under threshold in column: {num_values_under_085_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not neccessary\n",
    "\n",
    "with h5py.File('10_pairwise_correlation.h5', 'r') as hf:\n",
    "    dataset = hf['pairwise_correlation'][:]\n",
    "\n",
    "index = 0\n",
    "    \n",
    "# Count the number of non-zero values in the selected row\n",
    "row_i = dataset[index, :]\n",
    "col_i = dataset[:, index]\n",
    "\n",
    "\n",
    "selected_values = []\n",
    "\n",
    "for i, j in zip(row_i, col_i):\n",
    "    # Check if both values are non-zero and less than 0.99\n",
    "    if i != 0 and i < 0.99:\n",
    "        selected_values.append(i)\n",
    "    if j != 0 and j < 0.99:\n",
    "        selected_values.append(j)\n",
    "\n",
    "count_under_085 = sum(1 for value in selected_values if value < 0.95)\n",
    "\n",
    "print(f\"All values in row {index}: {len(selected_values)}\")\n",
    "print(f\"Values under threshold: {count_under_085}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b1c2cc",
   "metadata": {},
   "source": [
    "## Double checking the PC value in csv (two rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd58e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also for comparison, you can check \"small PC test\" section (upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca486e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"4_sorted_head.csv\")\n",
    "\n",
    "# Select the columns of interest (from column 23 onwards)\n",
    "gene_columns = df.columns[16:]\n",
    "df1 = df.loc[:, gene_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "\n",
    "row1 = df1.loc[0, gene_columns].astype(float)\n",
    "row2 = df1.loc[1, gene_columns].astype(float)\n",
    "\n",
    "\n",
    "non_nan_columns = row1.notna() & row2.notna()\n",
    "row1_dropped = row1[non_nan_columns]\n",
    "row2_dropped = row2[non_nan_columns]\n",
    "\n",
    "\n",
    "mean_row1 = row1_dropped.mean()\n",
    "mean_row2 = row2_dropped.mean()\n",
    "\n",
    "\n",
    "row1_minus_mean = row1_dropped - mean_row1\n",
    "row2_minus_mean = row2_dropped - mean_row2\n",
    "\n",
    "sum_of_products_deviations = (row1_minus_mean * row2_minus_mean).sum()\n",
    "\n",
    "std_row1 = row1_dropped.std()\n",
    "std_row2 = row2_dropped.std()\n",
    "\n",
    "PC0 = sum_of_products_deviations / (std_row1 * std_row2) \n",
    "PC = PC0 / len(row2_dropped)\n",
    "    \n",
    "print(f\"PC between Row 1 and Row 2: {PC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f267452",
   "metadata": {},
   "source": [
    "## Histogram of pairwise PC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719dac8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10_pairwise_correlation.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hf:\n\u001b[1;32m      2\u001b[0m     pairwise_correlation \u001b[38;5;241m=\u001b[39m hf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpairwise_correlation\u001b[39m\u001b[38;5;124m'\u001b[39m][:]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Flatten the matrix into a 1D array\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "with h5py.File('10_pairwise_correlation.h5', 'r') as hf:\n",
    "    pairwise_correlation = hf['pairwise_correlation'][:]\n",
    "\n",
    "# Flatten the matrix into a 1D array\n",
    "values = pairwise_correlation.flatten()\n",
    "\n",
    "# Remove zeros from the array\n",
    "non_zero_values = values[values != 0]\n",
    "\n",
    "# Create a histogram of the non-zero values\n",
    "plt.hist(non_zero_values, bins=80, ec=\"purple\")\n",
    "plt.xlabel('Pearson Coefficients')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Pairwise Pearson correlation among replicates (Sigcom A375)')\n",
    "#plt.savefig('11_histogram_A375_before.png', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46604e",
   "metadata": {},
   "source": [
    "# H5 modification (indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0a9bd",
   "metadata": {},
   "source": [
    "## Adding indices to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute separately on the server\n",
    "\n",
    "with h5py.File('10_pairwise_correlation.h5', 'r') as hf:\n",
    "    # Get the dataset (assuming your dataset is named 'correlation')\n",
    "    dataset = hf['pairwise_correlation'][:]\n",
    "    \n",
    "    # Get the shape of the dataset\n",
    "    num_rows, num_columns = dataset.shape\n",
    "    \n",
    "    # Create a new array containing the indices of the columns\n",
    "    column_indices = np.arange(num_columns, dtype=int)\n",
    "    \n",
    "    # Append the new row to the matrix\n",
    "    dataset_with_indices = np.vstack((dataset, column_indices))\n",
    "    \n",
    "# Save the updated matrix with indices to the h5 file\n",
    "with h5py.File('13_pairwise_correlation_indices.h5', 'w') as hf:\n",
    "    hf.create_dataset('pairwise_correlation', data=dataset_with_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9d9f1",
   "metadata": {},
   "source": [
    "## How many values below threshold? & checking last line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check with histogram too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "with h5py.File(\"13_pairwise_correlation_indices.h5\", \"r\") as hf:\n",
    "    dset = hf[\"pairwise_correlation\"]\n",
    "    last_row = dset[-1,:]\n",
    "    rows, cols = dset.shape\n",
    "    \n",
    "    # Define chunk size (you may need to adjust this based on your memory)\n",
    "    chunk_rows = 5000\n",
    "    threshold = 0.92\n",
    "    \n",
    "    for start_row in range(0, rows, chunk_rows):\n",
    "        end_row = min(rows, start_row + chunk_rows)\n",
    "        \n",
    "        # Read a chunk of data into memory\n",
    "        chunk_data = dset[start_row:end_row, :]\n",
    "        \n",
    "        # Update the counter\n",
    "        count += np.sum((chunk_data != 0) & (chunk_data < threshold))\n",
    "\n",
    "print(\"last row of h5 file:\")\n",
    "print(last_row, \"\\n\")\n",
    "print(f\"Number of non-zero values below 0.92: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec75dfd",
   "metadata": {},
   "source": [
    "# H5 below threshold cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670ad43",
   "metadata": {},
   "source": [
    "## 1. txt creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcaa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find them (the ones with the most num of correlations below the threshold), delete them. Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('13_pairwise_correlation_indices.h5', 'r') as hf:\n",
    "    dataset = hf['pairwise_correlation'][:]\n",
    "\n",
    "selected_samples = []\n",
    "\n",
    "def count(index, threshold, number):\n",
    "    row_i = dataset[index, :]\n",
    "    col_i = dataset[:, index]\n",
    "\n",
    "    # Select non-zero values for row_i and col_i\n",
    "    row_values = row_i[row_i != 0]\n",
    "    col_values = col_i[col_i != 0]\n",
    "    \n",
    "    # Concatenate the selected non-zero values\n",
    "    selected_values = np.concatenate([row_values, col_values])\n",
    "\n",
    "    count_under_threshold = np.count_nonzero(selected_values < threshold)\n",
    "    if count_under_threshold >= number:\n",
    "        selected_samples.append(index)\n",
    "\n",
    "for i in range(dataset.shape[0]-1):\n",
    "    count(i, 0.9, 1)\n",
    "\n",
    "print(len(selected_samples))\n",
    "print(selected_samples)\n",
    "np.savetxt(\"14.txt\", selected_samples, fmt='%d')\n",
    "\n",
    "#selected_samples = np.loadtxt(\"selected_samples.txt\", dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661eee6",
   "metadata": {},
   "source": [
    "## 2. removing samples in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a5590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the indices from the text file into a list\n",
    "with open(\"14.txt\", \"r\") as file:\n",
    "    selected_samples = [int(line.strip()) for line in file]\n",
    "\n",
    "# Step 2: Open the h5 file in read-write mode\n",
    "with h5py.File(\"40.h5\", \"r\") as hf:\n",
    "    # Step 3: Remove the rows and columns from the dataset\n",
    "    correlation_matrix = hf['pairwise_correlation'][:]\n",
    "    filtered_correlation_matrix0 = np.delete(correlation_matrix, selected_samples, axis=0)\n",
    "    filtered_correlation_matrix = np.delete(filtered_correlation_matrix0, selected_samples, axis=1)\n",
    "\n",
    "with h5py.File('41_clean_dataset.h5', 'w') as hf:\n",
    "    hf.create_dataset('pairwise_correlation', data=filtered_correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32364e3b",
   "metadata": {},
   "source": [
    "## 3. Checkinf rows and cols of h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ce935",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('13_pairwise_correlation_indices.h5', 'r') as hf:\n",
    "    correlation_matrix = hf['pairwise_correlation'][:]\n",
    "\n",
    "print(correlation_matrix.shape)\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecebfca",
   "metadata": {},
   "source": [
    "## 4. New Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('50_Cleaned_Data_indices.h5', 'r') as hf:\n",
    "    pairwise_correlation = hf['correlation'][:]\n",
    "\n",
    "# Flatten the matrix into a 1D array\n",
    "values = pairwise_correlation.flatten()\n",
    "\n",
    "# Remove zeros from the array\n",
    "non_zero_values = values[values != 0]\n",
    "\n",
    "# Create a histogram of the non-zero values\n",
    "plt.hist(non_zero_values, bins=100, ec=\"purple\")\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel('Pearson Coefficients')\n",
    "#plt.ylabel('Count')\n",
    "plt.title('Pairwise Pearson correlation among replicates_cleaned (MCF7)')\n",
    "#plt.savefig('42_histogram_clean_MCF7.png', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82ff83",
   "metadata": {},
   "source": [
    "# Cleaning original CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7419b3e",
   "metadata": {},
   "source": [
    "## 1. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('50_Cleaned_Data_indices.h5', 'r') as hf:\n",
    "    \n",
    "    # Assuming your dataset is named 'matrix', get the last row\n",
    "    indices_to_keep = hf['correlation'][-1,:]\n",
    "    \n",
    "df = pd.read_csv('9_sorted_dataset.csv')\n",
    "\n",
    "# Filter the DataFrame to keep only the rows with the specified indices\n",
    "filtered_df = df[df.index.isin(indices_to_keep)]\n",
    "\n",
    "# Add a new column with the old indices\n",
    "filtered_df.insert(0, 'old_index', filtered_df.index)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv('51_MCF7_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe34a7b",
   "metadata": {},
   "source": [
    "## 2. Checking rows and cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('51_MCF7_clean.csv')\n",
    "\n",
    "print(df.shape)\n",
    "#print(f\"Number of rows in clean csv file: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a9681",
   "metadata": {},
   "source": [
    "## 3. Making clean_head file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_dataset.csv')\n",
    "filtered_df = df.head(200)\n",
    "filtered_df.to_csv('52_head.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0724af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary\n",
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "print(df[\"A4GNT\"])\n",
    "print(df[\"ATP5G2P3\"])\n",
    "print(df[\"HCRTR2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8adaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "\n",
    "# Extract the gene expression values from columns 23 onwards\n",
    "gene_expression_df = df.iloc[:, 16:]\n",
    "\n",
    "# Get the number of columns from 23 onward\n",
    "num_columns_from_23 = gene_expression_df.shape[1]\n",
    "\n",
    "# Get the unique gene names from column 23 onward\n",
    "unique_gene_names = gene_expression_df.columns.unique()\n",
    "num_unique_gene_names = len(unique_gene_names)\n",
    "\n",
    "print(f\"Number of columns from column 16 onward: {num_columns_from_23}\")\n",
    "print(f\"Number of unique gene names: {num_unique_gene_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a9c7d",
   "metadata": {},
   "source": [
    "## 4. Printing last 50 cols (genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165febd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "print(df.columns[-30:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159b0dc",
   "metadata": {},
   "source": [
    "## 5. Getting loc of a specific col (gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5_sorted_head.csv')\n",
    "\n",
    "# Find the index of the column \"cpd\"\n",
    "cpd_index = df.columns.get_loc(\"cid\")\n",
    "\n",
    "print(cpd_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed01300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8443f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d609d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f468d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2142fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ee12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0b36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b4410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd574f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c4655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed4e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b66932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
